{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0894ed45-d62c-420f-a81c-25ebc07d7a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1: https://www.zameen.com/Homes/Lahore-1-1.html\n",
      "Fetching page 2: https://www.zameen.com/Homes/Lahore-1-2.html\n",
      "Fetching page 3: https://www.zameen.com/Homes/Lahore-1-3.html\n",
      "Fetching page 4: https://www.zameen.com/Homes/Lahore-1-4.html\n",
      "Fetching page 5: https://www.zameen.com/Homes/Lahore-1-5.html\n",
      "Fetching page 6: https://www.zameen.com/Homes/Lahore-1-6.html\n",
      "Fetching page 7: https://www.zameen.com/Homes/Lahore-1-7.html\n",
      "Fetching page 8: https://www.zameen.com/Homes/Lahore-1-8.html\n",
      "Fetching page 9: https://www.zameen.com/Homes/Lahore-1-9.html\n",
      "Fetching page 10: https://www.zameen.com/Homes/Lahore-1-10.html\n",
      "Fetching page 11: https://www.zameen.com/Homes/Lahore-1-11.html\n",
      "Fetching page 12: https://www.zameen.com/Homes/Lahore-1-12.html\n",
      "Fetching page 13: https://www.zameen.com/Homes/Lahore-1-13.html\n",
      "Fetching page 14: https://www.zameen.com/Homes/Lahore-1-14.html\n",
      "Fetching page 15: https://www.zameen.com/Homes/Lahore-1-15.html\n",
      "Fetching page 16: https://www.zameen.com/Homes/Lahore-1-16.html\n",
      "Fetching page 17: https://www.zameen.com/Homes/Lahore-1-17.html\n",
      "Fetching page 18: https://www.zameen.com/Homes/Lahore-1-18.html\n",
      "Fetching page 19: https://www.zameen.com/Homes/Lahore-1-19.html\n",
      "Fetching page 20: https://www.zameen.com/Homes/Lahore-1-20.html\n",
      "Fetching page 21: https://www.zameen.com/Homes/Lahore-1-21.html\n",
      "Fetching page 22: https://www.zameen.com/Homes/Lahore-1-22.html\n",
      "Fetching page 23: https://www.zameen.com/Homes/Lahore-1-23.html\n",
      "Fetching page 24: https://www.zameen.com/Homes/Lahore-1-24.html\n",
      "Fetching page 25: https://www.zameen.com/Homes/Lahore-1-25.html\n",
      "Fetching page 26: https://www.zameen.com/Homes/Lahore-1-26.html\n",
      "Fetching page 27: https://www.zameen.com/Homes/Lahore-1-27.html\n",
      "Fetching page 28: https://www.zameen.com/Homes/Lahore-1-28.html\n",
      "Fetching page 29: https://www.zameen.com/Homes/Lahore-1-29.html\n",
      "Fetching page 30: https://www.zameen.com/Homes/Lahore-1-30.html\n",
      "Fetching page 31: https://www.zameen.com/Homes/Lahore-1-31.html\n",
      "Fetching page 32: https://www.zameen.com/Homes/Lahore-1-32.html\n",
      "Fetching page 33: https://www.zameen.com/Homes/Lahore-1-33.html\n",
      "Fetching page 34: https://www.zameen.com/Homes/Lahore-1-34.html\n",
      "Fetching page 35: https://www.zameen.com/Homes/Lahore-1-35.html\n",
      "Fetching page 36: https://www.zameen.com/Homes/Lahore-1-36.html\n",
      "Fetching page 37: https://www.zameen.com/Homes/Lahore-1-37.html\n",
      "Fetching page 38: https://www.zameen.com/Homes/Lahore-1-38.html\n",
      "Fetching page 39: https://www.zameen.com/Homes/Lahore-1-39.html\n",
      "Fetching page 40: https://www.zameen.com/Homes/Lahore-1-40.html\n",
      "Fetching page 41: https://www.zameen.com/Homes/Lahore-1-41.html\n",
      "Fetching page 42: https://www.zameen.com/Homes/Lahore-1-42.html\n",
      "Fetching page 43: https://www.zameen.com/Homes/Lahore-1-43.html\n",
      "Fetching page 44: https://www.zameen.com/Homes/Lahore-1-44.html\n",
      "Fetching page 45: https://www.zameen.com/Homes/Lahore-1-45.html\n",
      "Fetching page 46: https://www.zameen.com/Homes/Lahore-1-46.html\n",
      "Fetching page 47: https://www.zameen.com/Homes/Lahore-1-47.html\n",
      "Fetching page 48: https://www.zameen.com/Homes/Lahore-1-48.html\n",
      "Fetching page 49: https://www.zameen.com/Homes/Lahore-1-49.html\n",
      "Fetching page 50: https://www.zameen.com/Homes/Lahore-1-50.html\n",
      "Fetching page 51: https://www.zameen.com/Homes/Lahore-1-51.html\n",
      "Fetching page 52: https://www.zameen.com/Homes/Lahore-1-52.html\n",
      "Fetching page 53: https://www.zameen.com/Homes/Lahore-1-53.html\n",
      "Fetching page 54: https://www.zameen.com/Homes/Lahore-1-54.html\n",
      "Fetching page 55: https://www.zameen.com/Homes/Lahore-1-55.html\n",
      "Fetching page 56: https://www.zameen.com/Homes/Lahore-1-56.html\n",
      "Fetching page 57: https://www.zameen.com/Homes/Lahore-1-57.html\n",
      "Fetching page 58: https://www.zameen.com/Homes/Lahore-1-58.html\n",
      "Fetching page 59: https://www.zameen.com/Homes/Lahore-1-59.html\n",
      "Fetching page 60: https://www.zameen.com/Homes/Lahore-1-60.html\n",
      "Fetching page 61: https://www.zameen.com/Homes/Lahore-1-61.html\n",
      "Fetching page 62: https://www.zameen.com/Homes/Lahore-1-62.html\n",
      "Fetching page 63: https://www.zameen.com/Homes/Lahore-1-63.html\n",
      "Fetching page 64: https://www.zameen.com/Homes/Lahore-1-64.html\n",
      "Fetching page 65: https://www.zameen.com/Homes/Lahore-1-65.html\n",
      "Fetching page 66: https://www.zameen.com/Homes/Lahore-1-66.html\n",
      "Fetching page 67: https://www.zameen.com/Homes/Lahore-1-67.html\n",
      "Fetching page 68: https://www.zameen.com/Homes/Lahore-1-68.html\n",
      "Fetching page 69: https://www.zameen.com/Homes/Lahore-1-69.html\n",
      "Fetching page 70: https://www.zameen.com/Homes/Lahore-1-70.html\n",
      "Fetching page 71: https://www.zameen.com/Homes/Lahore-1-71.html\n",
      "Fetching page 72: https://www.zameen.com/Homes/Lahore-1-72.html\n",
      "Fetching page 73: https://www.zameen.com/Homes/Lahore-1-73.html\n",
      "Fetching page 74: https://www.zameen.com/Homes/Lahore-1-74.html\n",
      "Fetching page 75: https://www.zameen.com/Homes/Lahore-1-75.html\n",
      "Fetching page 76: https://www.zameen.com/Homes/Lahore-1-76.html\n",
      "Fetching page 77: https://www.zameen.com/Homes/Lahore-1-77.html\n",
      "Fetching page 78: https://www.zameen.com/Homes/Lahore-1-78.html\n",
      "Fetching page 79: https://www.zameen.com/Homes/Lahore-1-79.html\n",
      "Fetching page 80: https://www.zameen.com/Homes/Lahore-1-80.html\n",
      "Fetching page 81: https://www.zameen.com/Homes/Lahore-1-81.html\n",
      "Fetching page 82: https://www.zameen.com/Homes/Lahore-1-82.html\n",
      "Fetching page 83: https://www.zameen.com/Homes/Lahore-1-83.html\n",
      "Fetching page 84: https://www.zameen.com/Homes/Lahore-1-84.html\n",
      "Fetching page 85: https://www.zameen.com/Homes/Lahore-1-85.html\n",
      "Fetching page 86: https://www.zameen.com/Homes/Lahore-1-86.html\n",
      "Fetching page 87: https://www.zameen.com/Homes/Lahore-1-87.html\n",
      "Fetching page 88: https://www.zameen.com/Homes/Lahore-1-88.html\n",
      "Fetching page 89: https://www.zameen.com/Homes/Lahore-1-89.html\n",
      "Fetching page 90: https://www.zameen.com/Homes/Lahore-1-90.html\n",
      "Fetching page 91: https://www.zameen.com/Homes/Lahore-1-91.html\n",
      "Fetching page 92: https://www.zameen.com/Homes/Lahore-1-92.html\n",
      "Fetching page 93: https://www.zameen.com/Homes/Lahore-1-93.html\n",
      "Fetching page 94: https://www.zameen.com/Homes/Lahore-1-94.html\n",
      "Fetching page 95: https://www.zameen.com/Homes/Lahore-1-95.html\n",
      "Fetching page 96: https://www.zameen.com/Homes/Lahore-1-96.html\n",
      "Fetching page 97: https://www.zameen.com/Homes/Lahore-1-97.html\n",
      "Fetching page 98: https://www.zameen.com/Homes/Lahore-1-98.html\n",
      "Fetching page 99: https://www.zameen.com/Homes/Lahore-1-99.html\n",
      "Fetching page 100: https://www.zameen.com/Homes/Lahore-1-100.html\n",
      "\n",
      "Collected 2489 unique detail page URLs.\n",
      "Starting concurrent processing of detail pages...\n",
      "\n",
      "âœ… Data saved to 'zameen_listings_final_All.csv'.\n"
     ]
    }
   ],
   "source": [
    "#ADDED THE MULTITHREADING FUNCTIONALITY, PROCESSING SPEED HAS IMPROVED CONSIDERABLY, 100 PAGES (~2500 LISTINGS) SCRAPED IN AROUND 25 MINS\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "\n",
    "# making the scraper behave like a standard web browser. \n",
    "BASE_URL = \"https://www.zameen.com\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"close\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "}\n",
    "\n",
    "# Functions\n",
    "def convert_price(price_str):\n",
    "    price_str = str(price_str).replace(\",\", \"\").strip()\n",
    "    if not price_str: return 0.0\n",
    "    if \"Crore\" in price_str: return round(float(price_str.replace('Crore', '').strip()) * 10_000_000)\n",
    "    elif \"Lakh\" in price_str: return round(float(price_str.replace('Lakh', '').strip()) * 100_000)\n",
    "    elif \"Million\" in price_str: return round(float(price_str.replace('Million', '').strip()) * 1_000_000)\n",
    "    elif \"Arab\" in price_str: return round(float(price_str.replace('Arab', '').strip()) * 1_000_000_000)\n",
    "    elif \"Thousand\" in price_str: return round(float(price_str.replace('Thousand', '').strip()) * 1_000)\n",
    "    else:\n",
    "        try: return round(float(re.sub(r'[^\\d.]', '', price_str)))\n",
    "        except ValueError: return 0.0\n",
    "\n",
    "def convert_size(size_str):\n",
    "    size_str = str(size_str).replace(\",\", \"\").strip()\n",
    "    if not size_str: return 0.0\n",
    "    if \"Marla\" in size_str: return round(float(size_str.replace('Marla', '').strip()))\n",
    "    elif \"Kanal\" in size_str: return round(float(size_str.replace('Kanal', '').strip()) * 20)\n",
    "    elif \"Sq. Yd.\" in size_str: return round(float(size_str.replace('Sq. Yd.', '').strip()) * (9 / 225), 2)\n",
    "    else:\n",
    "        try: return round(float(size_str) / 225, 2)\n",
    "        except ValueError: return 0.0\n",
    "\n",
    "def convert_relative_date_to_absolute(relative_date_str):\n",
    "    if not relative_date_str: return None\n",
    "    relative_date_str = relative_date_str.lower().strip()\n",
    "    now = datetime.now()\n",
    "    if \"just now\" in relative_date_str or \"few seconds ago\" in relative_date_str: return now.strftime('%Y-%m-%d')\n",
    "    elif \"yesterday\" in relative_date_str: return (now - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    parts = relative_date_str.split()\n",
    "    if len(parts) < 2: return None\n",
    "    try: value = int(parts[0]); unit = parts[1]\n",
    "    except ValueError: return None\n",
    "    delta = None\n",
    "    if \"minute\" in unit: delta = timedelta(minutes=value)\n",
    "    elif \"hour\" in unit: delta = timedelta(hours=value)\n",
    "    elif \"day\" in unit: delta = timedelta(days=value)\n",
    "    elif \"week\" in unit: delta = timedelta(weeks=value)\n",
    "    elif \"month\" in unit: delta = timedelta(days=value * 30)\n",
    "    elif \"year\" in unit: delta = timedelta(days=value * 365)\n",
    "    if delta: return (now - delta).strftime('%Y-%m-%d')\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da026bd-97d2-4090-a417-2ff6492d2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Scraping Function\n",
    "\n",
    "def get_detail_page_data_from_dataLayer(detail_url):\n",
    "    \"\"\"\n",
    "    Extracts dataLayer object from a detail page. Re-added robust error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(detail_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        dataLayer_pattern = re.compile(r\"window\\['dataLayer'\\]\\.push\\((\\{.*?\\})\\);\", re.DOTALL) \n",
    "        match = dataLayer_pattern.search(response.text)\n",
    "\n",
    "        if match:\n",
    "            json_str = match.group(1) \n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                return data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"  Error decoding dataLayer JSON for {detail_url}: {e}\")\n",
    "        else:\n",
    "            print(f\"  window['dataLayer'] push pattern not found in {detail_url}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching detail page {detail_url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {detail_url}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_listing_links(page):\n",
    "    \"\"\"\n",
    "    Extracts unique property detail page URLs from a Zameen.com search result page.\n",
    "    Re-added robust error handling.\n",
    "    \"\"\"\n",
    "    search_url = f'https://www.zameen.com/Homes/Lahore-1-{page}.html'\n",
    "    print(f\"Fetching page {page}: {search_url}\")\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a', attrs={'aria-label': 'Listing link'}, href=re.compile(r'/Property/.*\\.html')):\n",
    "            href = a_tag['href']; full_url = BASE_URL + href\n",
    "            if full_url not in links: links.append(full_url)\n",
    "        \n",
    "        if not links:\n",
    "            print(f\"  No new listing links found on page {page}. This might indicate end of results or a change in URL patterns.\")\n",
    "\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching listing page {search_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_html_details(soup, data_layer_data):\n",
    "    \"\"\"\n",
    "    Extracts core property details from the parsed HTML (BeautifulSoup object)\n",
    "    and augments with data from the dataLayer. Re-added robust error handling.\n",
    "    \"\"\"\n",
    "    details = { 'Ad_ID': None, 'Price': None, 'Area': None, 'Bedrooms': None, 'Bathrooms': None,\n",
    "                'Latitude': None, 'Longitude': None, 'Property_Type': None, \n",
    "                'Location_Detail': None, 'Date_Added': None }\n",
    "\n",
    "    # Ad ID from dataLayer (NEW) - kept robust checks\n",
    "    if data_layer_data and data_layer_data.get('ad_id'):\n",
    "        details['Ad_ID'] = data_layer_data['ad_id']\n",
    "\n",
    "    # Price\n",
    "    price_tag = soup.find('span', attrs={'aria-label': 'Price'})\n",
    "    price_raw = price_tag.text.strip() if price_tag else None \n",
    "    details['Price'] = convert_price(price_raw)\n",
    "\n",
    "    # Bedrooms\n",
    "    bedrooms_tag = soup.find('span', attrs={'aria-label': 'Beds'})\n",
    "    bedrooms_raw = bedrooms_tag.text.strip() if bedrooms_tag else None\n",
    "    if bedrooms_raw:\n",
    "        beds_match = re.search(r'(\\d+)', bedrooms_raw)\n",
    "        details['Bedrooms'] = int(beds_match.group(1)) if beds_match else None\n",
    "    \n",
    "    # Bathrooms\n",
    "    bathrooms_tag = soup.find('span', attrs={'aria-label': 'Baths'})\n",
    "    bathrooms_raw = bathrooms_tag.text.strip() if bathrooms_tag else None \n",
    "    if bathrooms_raw:\n",
    "        baths_match = re.search(r'(\\d+)', bathrooms_raw)\n",
    "        details['Bathrooms'] = int(baths_match.group(1)) if baths_match else None \n",
    "        \n",
    "    # Area\n",
    "    area_tag = soup.find('span', attrs={'aria-label': 'Area'})\n",
    "    area_raw = area_tag.text.strip() if area_tag else None\n",
    "    details['Area'] = convert_size(area_raw)\n",
    "\n",
    "    # Property Type\n",
    "    property_type_tag = soup.find('span', attrs={'aria-label': 'Type'})\n",
    "    details['Property_Type'] = property_type_tag.text.strip() if property_type_tag else None \n",
    "\n",
    "    # Date Added\n",
    "    creation_date_tag = soup.find('span', attrs={'aria-label': 'Creation date'})\n",
    "    creation_date_raw = creation_date_tag.text.strip() if creation_date_tag else None \n",
    "    details['Date_Added'] = convert_relative_date_to_absolute(creation_date_raw)\n",
    "\n",
    "    # Latitude, Longitude, Location_Detail\n",
    "    if data_layer_data:\n",
    "        details['Latitude'] = float(data_layer_data.get('latitude')) if data_layer_data.get('latitude') is not None else None\n",
    "        details['Longitude'] = float(data_layer_data.get('longitude')) if data_layer_data.get('longitude') is not None else None\n",
    "        loc_components = []\n",
    "        if data_layer_data.get('loc_neighbourhood_name'): loc_components.append(data_layer_data['loc_neighbourhood_name'])\n",
    "        if data_layer_data.get('loc_name') and data_layer_data.get('loc_name') != data_layer_data.get('loc_neighbourhood_name'): loc_components.append(data_layer_data['loc_name'])\n",
    "        if data_layer_data.get('loc_city_name') and data_layer_data.get('loc_city_name') not in loc_components: loc_components.append(data_layer_data['loc_city_name'])\n",
    "        details['Location_Detail'] = ', '.join(loc_components).strip() if loc_components else None\n",
    "    return details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7028c23-9384-43a9-b106-60636757917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main Execution Function\n",
    "def main():\n",
    "    all_results = []\n",
    "    max_pages = 100 # Number of listing pages to scrape\n",
    "    target_city_slug = 'Lahore-1'\n",
    "\n",
    "    all_detail_urls = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        listing_links = get_listing_links(page)\n",
    "        all_detail_urls.extend(listing_links)\n",
    "        time.sleep(2) \n",
    "\n",
    "    print(f\"\\nCollected {len(all_detail_urls)} unique detail page URLs.\")\n",
    "    print(\"Starting concurrent processing of detail pages...\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_url = {executor.submit(process_single_listing, url): url for url in all_detail_urls}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result_dict = future.result() \n",
    "                if result_dict: \n",
    "                    all_results.append(result_dict)\n",
    "            except Exception as exc:\n",
    "                print(f'Error processing {url}: {exc}')\n",
    "                all_results.append({\n",
    "                    'URL': url, 'Ad_ID': None, 'Price': None, 'Area': None, \n",
    "                    'Bedrooms': None, 'Bathrooms': None, 'Latitude': None, \n",
    "                    'Longitude': None, 'Property_Type': None, \n",
    "                    'Location_Detail': None, 'Date_Added': None\n",
    "                })\n",
    "            time.sleep(0.1) \n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    desired_columns = [\n",
    "        'Ad_ID', 'Property_Type', 'Location_Detail', 'Price', 'Area', \n",
    "        'Bedrooms', 'Bathrooms', 'Latitude', 'Longitude', 'Date_Added', 'URL'\n",
    "    ]\n",
    "    for col in desired_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    df = df[desired_columns]\n",
    "\n",
    "    if 'URL' in df.columns:\n",
    "        df = df.drop(columns=['URL'])\n",
    "    \n",
    "    output_filename = 'zameen_listings_final_All.csv'\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nâœ… Data saved to '{output_filename}'.\")\n",
    "\n",
    "# Helper function for Multithreading\n",
    "def process_single_listing(url):\n",
    "\n",
    "    try:\n",
    "        data_layer_data = get_detail_page_data_from_dataLayer(url)\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        details = parse_html_details(soup, data_layer_data)\n",
    "        details['URL'] = url \n",
    "        return details\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  Thread Error: HTTP/Network issue for {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Thread Error: Unexpected error processing {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
